{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parallelization_demo_model as pdm\n",
    "import time\n",
    "from typing import *\n",
    "\n",
    "\n",
    "# read in data for random n\n",
    "df_random_n = pd.read_csv(\"random_n.csv\")\n",
    "# set the number of values to iterate over \n",
    "n_iter = 20\n",
    "n_iter = min(n_iter, len(df_random_n))\n",
    "\n",
    "# setup some fields \n",
    "field_id = \"random_n_id\"\n",
    "field_output = \"function_calc\"\n",
    "field_random_n = \"random_n\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.634175062179565"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstration run using a large n\n",
    "t0 = time.time()\n",
    "n = 5000\n",
    "rv = pdm.log_sum_binomial(n, None)\n",
    "time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1 iterations complete\n",
      "\t5 iterations complete\n",
      "\t9 iterations complete\n",
      "\t13 iterations complete\n",
      "\t17 iterations complete\n",
      "Serial run complete in 77.94128203392029 seconds.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "#    SERIAL 'FOR' LOOP    #\n",
    "###########################\n",
    "\n",
    "# initialize output values\n",
    "vec_logsums = [0 for x in range(len(df_random_n))]\n",
    "\n",
    "# set timer baseline\n",
    "t0_serial = time.time()\n",
    "\n",
    "# simple loop over\n",
    "for i in range(n_iter):\n",
    "    # important for this model to conver to int based on numerical issues\n",
    "    vec_logsums[i] = pdm.log_sum_binomial(int(df_random_n[field_random_n].iloc[i]), int(df_random_n[field_id].iloc[i]))\n",
    "    if i%round(n_iter/5) == 0:\n",
    "        print(\"\\t%s iterations complete\"%(i + 1))\n",
    "\n",
    "t1_serial = time.time()\n",
    "t_elapse_serial = t1_serial - t0_serial\n",
    "\n",
    "print(\"Serial run complete in %s seconds.\"%t_elapse_serial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asynchronous parallelization across 14 cores reduced computational time by 87.05%.\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#    ASYNCHRONOUS PARALLEL    #\n",
    "###############################\n",
    "\n",
    "t0_par_async = time.time()\n",
    "\n",
    "#\n",
    "# SOLUTION TO GET APPLY_ASYNC TO WORK WITH JUPYTER LAB: FUNCTION HAS TO BE PLACED IN MODULE AND IMPORTED: https://stackoverflow.com/questions/47313732/jupyter-notebook-never-finishes-processing-using-multiprocessing-python-3\n",
    "# https://towardsdatascience.com/asynchronous-parallel-programming-in-python-with-multiprocessing-a3fc882b4023\n",
    "#\n",
    "\n",
    "# initialize output vector/array (pre-allocate memory)\n",
    "vec_logsums_par_async = []\n",
    "\n",
    "# set up dummy functions to get results\n",
    "def _get_result(\n",
    "    result: Any,\n",
    ") -> None:\n",
    "    \n",
    "    global vec_logsums_par_async\n",
    "    \n",
    "    # update\n",
    "    vec_logsums_par_async.append(result)\n",
    "    \n",
    "    return None\n",
    "\n",
    "    \n",
    "\n",
    "# check to ensure current module is \"__main__\"; this is necessary in scripts that use multiprocessing. Without it, the processing framework will run the entirety of the original script in parallel\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # start the MP pool for asynchronous parallelization\n",
    "    pool = mp.Pool(int(mp.cpu_count()))\n",
    "\n",
    "    # apply the function; note: if the function only takes one argument (e.g., f(x)), make sure the args is args = (x, ) - that extra comma is important\n",
    "    for i in range(n_iter):\n",
    "        pool.apply_async(\n",
    "            # target function\n",
    "            pdm.log_sum_binomial,\n",
    "            # function arguments \n",
    "            args = (\n",
    "                int(df_random_n[field_random_n].iloc[i]), \n",
    "                int(df_random_n[field_id].iloc[i])\n",
    "            ),\n",
    "            callback = _get_result,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        pseudocode attempt at describing this function:\n",
    "            for i in 0:(n_iter - 1):\n",
    "                assign task i to Pool\n",
    "                use pdm.log_sum_binomial with arguments (int(df_random_n[field_random_n].iloc[i]), int(df_random_n[field_id].iloc[i]))\n",
    "                when task i finishes, apply _get_result to the output\n",
    "        \"\"\";\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    t1_par_async = time.time()\n",
    "\n",
    "    # \n",
    "    t_elapse_par_async = t1_par_async - t0_par_async\n",
    "\n",
    "# print the reduction in time\n",
    "print(\"Asynchronous parallelization across %s cores reduced computational time by %s%s.\"%(mp.cpu_count(), round(100*(1 - t_elapse_par_async/t_elapse_serial), 2), \"%\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 2796.848873559379),\n",
       " (11, 2842.5965874763356),\n",
       " (9, 2874.4813577820933),\n",
       " (13, 2909.83186399065),\n",
       " (2, 2959.0453138104062),\n",
       " (12, 3044.9955641998395),\n",
       " (5, 3055.392771908239),\n",
       " (6, 3058.8585078110386),\n",
       " (8, 3180.852411589589),\n",
       " (7, 3223.8275367843057),\n",
       " (3, 3259.1780429928626),\n",
       " (14, 3284.8244886735806),\n",
       " (1, 3332.651644132217),\n",
       " (10, 3366.615855979654),\n",
       " (15, 2974.9876989632853),\n",
       " (20, 2775.361310962021),\n",
       " (19, 2891.116890115532),\n",
       " (17, 3015.190235435762),\n",
       " (18, 3176.693528506229),\n",
       " (16, 3415.1361586188505)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine results from the pool, which gives us tuples with the random_n_id + the output value associated with it\n",
    "vec_logsums_par_async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2796.848873559379)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  verify the values shown above (interactive)\n",
    "# set the random id to check\n",
    "rand_id_check = 4\n",
    "# get the applicable data row\n",
    "row = df_random_n[df_random_n[field_id] == rand_id_check]\n",
    "#\n",
    "pdm.log_sum_binomial(int(row[field_random_n]), int(row[field_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synchronous parallelization across 12 cores reduced computational time by 67.7%.\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "#    SYNCHRONOUS PARALLEL    #\n",
    "##############################\n",
    "\n",
    "#\n",
    "# check to ensure current module is \"__main__\"; this is necessary in scripts that use multiprocessing. Without it, the processing framework will run the entirety of the original script in parallel\n",
    "# more on this is available at: https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming\n",
    "#\n",
    "# this approach is similar to running it in R\n",
    "#\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # start the MP pool for asynchronous parallelization\n",
    "    n_cores = mp.cpu_count()\n",
    "\n",
    "    # copy the range\n",
    "    list_task = list(range(n_iter))\n",
    "    \n",
    "    # set the outer return dictionary\n",
    "    return_values = {}\n",
    "    \n",
    "    # set kill timer\n",
    "    t0_par_sync = time.time()\n",
    "    # upper threshold\n",
    "    t_max = len(list_task)*20\n",
    "    \n",
    "    # set\n",
    "    while ((len(list_task) > 0) & (time.time() - t0_par_sync < t_max)):\n",
    "        \n",
    "        # initialize the manager\n",
    "        man = mp.Manager()\n",
    "        return_dict = man.dict()\n",
    "        \n",
    "        # initialize the list of processes\n",
    "        processes = []\n",
    "        list_task_drop = []\n",
    "        \n",
    "        # start processes on available cores\n",
    "        for i in range(min(n_cores, len(list_task))):\n",
    "            \n",
    "            # get the row index to work with\n",
    "            ind = list_task[i]\n",
    "            list_task_drop.append(ind)\n",
    "            \n",
    "            p = mp.Process(\n",
    "                target = pdm.log_sum_binomial_sync,\n",
    "                args = (int(df_random_n[field_random_n].iloc[ind]), int(df_random_n[field_id].iloc[ind]), return_dict)\n",
    "            )\n",
    "            \n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "            \n",
    "        # loop to close and join after starting\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        # update the return values\n",
    "        return_values.update(return_dict)\n",
    "        \n",
    "        # reduce the task list\n",
    "        list_task = [x for x in list_task if x not in list_task_drop]\n",
    "\n",
    "    t1_par_sync = time.time()\n",
    "\n",
    "    # \n",
    "    t_elapse_par_sync = t1_par_sync - t0_par_sync\n",
    "\n",
    "\n",
    "# print the reduction in time\n",
    "print(\"Synchronous parallelization across %s cores reduced computational time by %s%s.\"%(n_cores, round(100*(1 - t_elapse_par_sync/t_elapse_serial), 2), \"%\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177.7190670967102"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_elapse_serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.61187791824341"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_elapse_par_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.39674234390259"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_elapse_par_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try aligning the output to the input in a single data frame\n",
    "- use `pd.DataFrame` along with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool.all as gt\n",
    "g = gt.collection.ns[\"kangaroo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement graph_tool (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for graph_tool\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install graph_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2044189652.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[33], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    y = f(w1:w6) = w1x1 + w2x2 + w3x3 + w4x4 + w5x5 + w6x6 where (x1,x2,x3,x4,x5,x6)=(4,-2,3.5,5,-11,-4.7) and y=44\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "y = f(w1:w6) = w1x1 + w2x2 + w3x3 + w4x4 + w5x5 + w6x6 where (x1,x2,x3,x4,x5,x6)=(4,-2,3.5,5,-11,-4.7) and y=44"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
